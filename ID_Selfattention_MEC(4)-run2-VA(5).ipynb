{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d3f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mohammadreza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "Epoch 1/4\n",
      "Processing file: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\My Papers\\intrusion\\data generator\\pure_data\\pure_iq_samples_1.csv\n",
      "Step 1/2500, Loss: 1.009558916091919\n",
      "1/1 [==============================] - 1s 542ms/step\n",
      "Step 101/2500, Loss: 0.9997419118881226\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step 201/2500, Loss: 1.000754952430725\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step 301/2500, Loss: 1.0013219118118286\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step 401/2500, Loss: 1.0023374557495117\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Step 501/2500, Loss: 0.9996280670166016\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Step 601/2500, Loss: 1.0001800060272217\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 701/2500, Loss: 0.9997914433479309\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step 801/2500, Loss: 0.9999539256095886\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 901/2500, Loss: 0.9992561340332031\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step 1001/2500, Loss: 1.0000627040863037\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 1101/2500, Loss: 1.0001208782196045\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step 1201/2500, Loss: 0.9999826550483704\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step 1301/2500, Loss: 1.0000689029693604\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step 1401/2500, Loss: 1.0000358819961548\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 1501/2500, Loss: 1.0001214742660522\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 1601/2500, Loss: 0.9998553395271301\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 1701/2500, Loss: 1.0001041889190674\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step 1801/2500, Loss: 0.9995502233505249\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step 1901/2500, Loss: 1.0000221729278564\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 2001/2500, Loss: 0.9995499849319458\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 2101/2500, Loss: 0.9997488260269165\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Step 2201/2500, Loss: 1.00018310546875\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step 2301/2500, Loss: 1.0005152225494385\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step 2401/2500, Loss: 1.0001401901245117\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "\n",
      "Epoch 2/4\n",
      "Processing file: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\My Papers\\intrusion\\data generator\\pure_data\\pure_iq_samples_1.csv\n",
      "Step 1/2500, Loss: 0.9995427131652832\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Step 101/2500, Loss: 1.0000263452529907\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step 201/2500, Loss: 0.999942421913147\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 301/2500, Loss: 0.9999213218688965\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 401/2500, Loss: 0.999910831451416\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 501/2500, Loss: 1.0000970363616943\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step 601/2500, Loss: 1.000248908996582\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step 701/2500, Loss: 0.9997637867927551\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 801/2500, Loss: 1.0001072883605957\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 901/2500, Loss: 1.0001215934753418\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Step 1001/2500, Loss: 0.9999123811721802\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step 1101/2500, Loss: 0.9998830556869507\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step 1201/2500, Loss: 1.0000194311141968\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 1301/2500, Loss: 0.9999071359634399\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 1401/2500, Loss: 0.9999723434448242\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 1501/2500, Loss: 1.0001381635665894\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 1601/2500, Loss: 1.0000534057617188\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step 1701/2500, Loss: 1.000070333480835\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 1801/2500, Loss: 0.9998941421508789\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step 1901/2500, Loss: 0.9999638795852661\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Step 2001/2500, Loss: 0.9999720454216003\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 2101/2500, Loss: 1.0000295639038086\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Step 2201/2500, Loss: 1.0000016689300537\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Step 2301/2500, Loss: 0.9999648332595825\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 2401/2500, Loss: 0.9999418258666992\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "Epoch 3/4\n",
      "Processing file: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\My Papers\\intrusion\\data generator\\pure_data\\pure_iq_samples_1.csv\n",
      "Step 1/2500, Loss: 1.0000077486038208\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Step 101/2500, Loss: 1.00005304813385\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Step 201/2500, Loss: 1.0000327825546265\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 301/2500, Loss: 0.9999808073043823\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 401/2500, Loss: 1.0001182556152344\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 501/2500, Loss: 1.0000299215316772\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 601/2500, Loss: 1.0000455379486084\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Step 701/2500, Loss: 0.9999677538871765\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Step 801/2500, Loss: 0.9999779462814331\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step 901/2500, Loss: 0.9999620318412781\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Step 1001/2500, Loss: 1.000020980834961\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Step 1101/2500, Loss: 0.9999748468399048\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Step 1201/2500, Loss: 0.9999872446060181\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 1301/2500, Loss: 0.9999955296516418\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step 1401/2500, Loss: 1.0000067949295044\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step 1501/2500, Loss: 0.9999843835830688\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Step 1601/2500, Loss: 1.0000300407409668\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 1701/2500, Loss: 1.0000065565109253\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Step 1801/2500, Loss: 0.9999972581863403\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 1901/2500, Loss: 1.0\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 2001/2500, Loss: 0.9999980926513672\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step 2101/2500, Loss: 1.0000070333480835\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step 2201/2500, Loss: 1.000025749206543\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Step 2301/2500, Loss: 0.9999753832817078\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step 2401/2500, Loss: 0.9999763369560242\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "Epoch 4/4\n",
      "Processing file: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\My Papers\\intrusion\\data generator\\pure_data\\pure_iq_samples_1.csv\n",
      "Step 1/2500, Loss: 1.0001106262207031\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Step 101/2500, Loss: 1.0001046657562256\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Step 201/2500, Loss: 0.9999591112136841\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 301/2500, Loss: 0.99998939037323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "Step 401/2500, Loss: 0.9999129176139832\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 501/2500, Loss: 1.0000278949737549\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 601/2500, Loss: 0.9999862909317017\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 701/2500, Loss: 0.9999655485153198\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step 801/2500, Loss: 1.000022053718567\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step 901/2500, Loss: 1.0005985498428345\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 1001/2500, Loss: 0.999964714050293\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 1101/2500, Loss: 1.000016212463379\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step 1201/2500, Loss: 1.000054121017456\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step 1301/2500, Loss: 1.0000293254852295\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Step 1401/2500, Loss: 1.0000245571136475\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step 1501/2500, Loss: 1.0000489950180054\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step 1601/2500, Loss: 0.999960720539093\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Step 1701/2500, Loss: 1.000002145767212\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Layer, MultiHeadAttention\n",
    "from tensorflow.keras.layers import Masking, Input, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import mse\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import struct\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report, accuracy_score\n",
    "\n",
    "# Custom loss function\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    y_pred_clipped = tf.clip_by_value(y_pred, 1e-9, 1.0)  # Clip values to avoid log(0)\n",
    "    entropy_reg = -tf.reduce_mean(y_pred_clipped * tf.math.log(y_pred_clipped))\n",
    "    lambda_entropy = 0.01\n",
    "    return mse + lambda_entropy * entropy_reg\n",
    "\n",
    "def count_lines(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "class CSVDataGenerator:\n",
    "    def __init__(self, file_pattern, batch_size, sequence_length, max_samples=None, for_training=True):     \n",
    "        self.file_pattern = file_pattern\n",
    "        self.file_list = sorted(glob.glob(self.file_pattern))\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_samples = max_samples\n",
    "        self.for_training = for_training\n",
    "        self.current_file_idx = 0\n",
    "        self.dataframe_iterator = None\n",
    "        self.labels_iterator = None\n",
    "        self.samples_buffer = []\n",
    "        self.labels_buffer = []\n",
    "        self.total_samples_processed = 0\n",
    "\n",
    "    def _load_next_file(self):\n",
    "        if self.current_file_idx >= len(self.file_list):\n",
    "            print(\"No more files to process.\")\n",
    "            raise StopIteration\n",
    "\n",
    "        current_file = self.file_list[self.current_file_idx]\n",
    "        df = pd.read_csv(current_file)\n",
    "\n",
    "        # Filter out rows where 'IQ Data' is '0j'\n",
    "        df['IQ Data'] = df['IQ Data'].apply(lambda x: complex(x.replace('i', 'j')))\n",
    "        df = df[df['IQ Data'] != 0j]\n",
    "\n",
    "        # Check if DataFrame is empty or 'IQ Data' column is missing\n",
    "        if df.empty or 'IQ Data' not in df.columns:\n",
    "            raise ValueError(f\"File {current_file} is empty or missing 'IQ Data' column after filtering 0j.\")\n",
    "\n",
    "        # If not for training, extract the labels\n",
    "        if not self.for_training and 'label' in df.columns:\n",
    "            self.labels_iterator = iter(df['label'].map(lambda x: 1 if x == 'jammer' else 0).values)\n",
    "        else:\n",
    "            self.labels_iterator = None\n",
    "\n",
    "        print(f\"Processing file: {current_file}\")\n",
    "        self.dataframe_iterator = iter(df['IQ Data'].values)\n",
    "        self.current_file_idx += 1\n",
    "\n",
    "    # Reset function to be used when switching from training to prediction\n",
    "    def reset_for_prediction(self):\n",
    "        self.current_file_idx = 0\n",
    "        self.samples_buffer = []\n",
    "        self.labels_buffer = []\n",
    "        self.total_samples_processed = 0\n",
    "        self.dataframe_iterator = None\n",
    "        self._load_next_file()  # Start from the first file again\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_file_idx = 0\n",
    "        self.samples_buffer = []\n",
    "        self.labels_buffer = []\n",
    "        self.total_samples_processed = 0\n",
    "        self._load_next_file()\n",
    "        return self\n",
    "\n",
    "    def process_data(self, samples):\n",
    "        real_parts = np.real(samples)\n",
    "        imag_parts = np.imag(samples)\n",
    "\n",
    "        real_parts = (real_parts - np.mean(real_parts)) / np.std(real_parts)\n",
    "        imag_parts = (imag_parts - np.mean(imag_parts)) / np.std(imag_parts)\n",
    "\n",
    "        X = np.array(list(zip(real_parts, imag_parts))).reshape(-1, self.sequence_length, 2)\n",
    "        return X\n",
    "    def __next__(self):\n",
    "        if self.max_samples and self.total_samples_processed >= self.max_samples:\n",
    "            raise StopIteration(\"Reached max_samples limit.\")\n",
    "\n",
    "        while len(self.samples_buffer) < self.batch_size * self.sequence_length:\n",
    "            try:\n",
    "                chunk = next(self.dataframe_iterator)\n",
    "                self.samples_buffer.append(chunk)\n",
    "                if not self.for_training and self.labels_iterator is not None:\n",
    "                    label_chunk = next(self.labels_iterator)\n",
    "                    self.labels_buffer.append(label_chunk)\n",
    "            except StopIteration:\n",
    "                if self.current_file_idx >= len(self.file_list):\n",
    "                    raise StopIteration(\"No more data to process.\")\n",
    "                self._load_next_file()\n",
    "\n",
    "        samples = self.samples_buffer[:self.batch_size * self.sequence_length]\n",
    "        self.samples_buffer = self.samples_buffer[self.batch_size * self.sequence_length:]\n",
    "\n",
    "        X_chunk = self.process_data(np.array(samples))\n",
    "        #print('X_chunk.shape in next:', X_chunk.shape)\n",
    "\n",
    "        if not self.for_training:\n",
    "            #print('im in if not self.for_training')\n",
    "            labels = self.labels_buffer[:self.batch_size * self.sequence_length]\n",
    "            #print('X_chunk.shape:', X_chunk.shape)\n",
    "            self.labels_buffer = self.labels_buffer[self.batch_size * self.sequence_length:]\n",
    "            #print('X_chunk.shape:', X_chunk.shape)\n",
    "            return X_chunk, np.array(labels)\n",
    "        else:\n",
    "            #print('Im in else')\n",
    "            return X_chunk, X_chunk\n",
    "    def close(self):\n",
    "        self.samples_buffer = []\n",
    "        self.labels_buffer = []\n",
    "        self.total_samples_processed = 0\n",
    "        self.current_file_idx = 0\n",
    "        self.dataframe_iterator = None\n",
    "        self.labels_iterator = None \n",
    "#------------------------------------------------------------------------------------------------------\n",
    "#self Attention LSTM Autoencoder Model\n",
    "class SelfAttentionLayer(Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.multi_head_attention(inputs, inputs, inputs)\n",
    "\n",
    "sequence_length = 10\n",
    "\n",
    "# Minimum Entropy Coupling (MEC) Functions\n",
    "def mec_kocaoglu_np(p, q):\n",
    "    \"\"\"\n",
    "    Compute the joint distribution matrix with minimal entropy between two given distributions.\n",
    "    \"\"\"\n",
    "    p = tf.cast(p, tf.float64) / tf.reduce_sum(p)\n",
    "    q = tf.cast(q, tf.float64) / tf.reduce_sum(q)\n",
    "    J = tf.zeros((tf.size(q), tf.size(p)), dtype=tf.float64)\n",
    "    M = tf.stack([p, q], axis=0)\n",
    "    r = tf.reduce_min(tf.reduce_max(M, axis=1))\n",
    "    #print('Input shapes to mec_kocaoglu_np:', p.shape, q.shape)\n",
    "\n",
    "    def body(r, M, J):\n",
    "        a_i = tf.argmax(M, axis=1)\n",
    "        r_updated = tf.reduce_min(tf.reduce_max(M, axis=1))\n",
    "        update_values = tf.stack([r, r])\n",
    "\n",
    "        # Ensure tensors have the same data type before stacking\n",
    "        a_i = tf.cast(a_i, dtype=tf.int32)  # Cast to int32 (or choose another common dtype)\n",
    "        indices_range = tf.range(tf.size(a_i), dtype=tf.int32)  # Ensure range has the same dtype\n",
    "\n",
    "        # Prepare indices for scatter update\n",
    "        indices = tf.stack([indices_range, a_i], axis=1)\n",
    "\n",
    "        # Update M\n",
    "        M_updates = tf.scatter_nd(indices, -update_values, tf.shape(M))\n",
    "        M = M + M_updates\n",
    "\n",
    "        # Update J\n",
    "        J_updates = tf.scatter_nd(indices, [r, r], tf.shape(J))\n",
    "        J = J + J_updates\n",
    "\n",
    "        return r_updated, M, J\n",
    "\n",
    "    def condition(r, M, J):\n",
    "        return r > 0\n",
    "\n",
    "    r, M, J = tf.while_loop(condition, body, loop_vars=[r, M, J])\n",
    "\n",
    "    return J\n",
    "\n",
    "def apply_mec_to_data(data, num_bins=100, latent_dim=25):\n",
    "    \"\"\"\n",
    "    Apply the MEC transformation to each sample in the data using tf.map_fn.\n",
    "    \"\"\"\n",
    "    def process_sample(sample):\n",
    "        #print(\"Original sample shape:\", sample.shape)\n",
    "\n",
    "        min_val = tf.reduce_min(sample)\n",
    "        max_val = tf.reduce_max(sample)\n",
    "        sample_distribution = tf.histogram_fixed_width(sample, [min_val, max_val], nbins=num_bins)\n",
    "        sample_distribution = tf.cast(sample_distribution, tf.float64)\n",
    "        sum_distribution = tf.cast(tf.reduce_sum(sample_distribution), tf.float64)\n",
    "        sample_distribution /= sum_distribution\n",
    "\n",
    "        #print(\"Sample distribution shape:\", sample_distribution.shape)\n",
    "\n",
    "        mec_transformed = mec_kocaoglu_np(sample_distribution, sample_distribution)\n",
    "        #print(\"MEC transformed shape (initial):\", mec_transformed.shape)\n",
    "\n",
    "        # Flatten the 2D tensor to 1D if necessary\n",
    "        if len(mec_transformed.shape) > 1:\n",
    "            transformed_sample = tf.reshape(mec_transformed, [-1])\n",
    "            #print(\"Flattened MEC transformed shape:\", transformed_sample.shape)\n",
    "\n",
    "        # Slice or pad the tensor to match the latent_dim\n",
    "        if transformed_sample.shape[0] > latent_dim:\n",
    "            transformed_sample = transformed_sample[:latent_dim]\n",
    "        elif transformed_sample.shape[0] < latent_dim:\n",
    "            padding = tf.zeros(latent_dim - transformed_sample.shape[0], dtype=tf.float64)\n",
    "            transformed_sample = tf.concat([transformed_sample, padding], axis=0)\n",
    "\n",
    "        #print(\"Transformed sample shape (before reshape):\", transformed_sample.shape)\n",
    "\n",
    "        return tf.reshape(transformed_sample, (latent_dim,))\n",
    "\n",
    "\n",
    "    # Apply the function to each sample in the batch\n",
    "    transformed_batch = tf.map_fn(process_sample, data, dtype=tf.float64, parallel_iterations=10)\n",
    "\n",
    "    return transformed_batch\n",
    "\n",
    "# def apply_mec_to_data(data, num_bins=100, latent_dim=25):\n",
    "#     \"\"\"\n",
    "#     Apply the MEC transformation to the data.\n",
    "#     \"\"\"\n",
    "#     min_val = tf.reduce_min(data)\n",
    "#     max_val = tf.reduce_max(data)\n",
    "#     data_distribution = tf.histogram_fixed_width(data, [min_val, max_val], nbins=num_bins)\n",
    "#     data_distribution = tf.cast(data_distribution, tf.float64)\n",
    "#     data_distribution /= tf.reduce_sum(data_distribution)\n",
    "#     mec_transformed = mec_kocaoglu_np(data_distribution, data_distribution)\n",
    "\n",
    "#     # Reshape the output to match the latent dimension\n",
    "#     mec_transformed = tf.reduce_sum(mec_transformed, axis=0)\n",
    "#     mec_transformed = tf.reshape(mec_transformed, (-1, latent_dim))\n",
    "\n",
    "#     return mec_transformed\n",
    "\n",
    "def process_latent_variables(z):\n",
    "    \"\"\"\n",
    "    Process the latent variables with MEC transformation.\n",
    "    \"\"\"\n",
    "    # Apply MEC transformation to the latent variables\n",
    "    #print('Input to process_latent_variables shape:', z.shape)\n",
    "    z_transformed = apply_mec_to_data(z)\n",
    "    #print('Output of MEC transformation shape:', z_transformed.shape)\n",
    "    return z_transformed\n",
    "\n",
    "# Variational Autoencoder (VAE) Class\n",
    "class VAE:\n",
    "    def __init__(self, sequence_length, original_dim, intermediate_dim, latent_dim, epsilon_std=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the Variational Autoencoder (VAE) class.\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.original_dim = original_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vae = None\n",
    "        self._build()\n",
    "\n",
    "    def _sampling(self, args):\n",
    "        \"\"\"\n",
    "        Sampling function to generate samples from the latent space.\n",
    "        \"\"\"\n",
    "        z_mean, z_log_var = args\n",
    "\n",
    "        # Process latent variables with MEC transformation\n",
    "        z_mean_transformed = process_latent_variables(z_mean)\n",
    "        z_log_var_transformed = process_latent_variables(z_log_var)\n",
    "        #print('z_mean_transformed shape:', z_mean_transformed.shape)\n",
    "        #print('z_log_var_transformed shape:', z_log_var_transformed.shape)\n",
    "\n",
    "        #print('K.shape(z_mean_transformed):', K.shape(z_mean_transformed))\n",
    "        batch = K.shape(z_mean_transformed)[0]\n",
    "        #print('batch:', batch)\n",
    "        dim = K.int_shape(z_mean_transformed)[1]\n",
    "\n",
    "        # Ensure all tensors have the same data type\n",
    "        z_mean_transformed = tf.cast(z_mean_transformed, tf.float32)\n",
    "        z_log_var_transformed = tf.cast(z_log_var_transformed, tf.float32)\n",
    "\n",
    "        epsilon = K.random_normal(shape=(batch, dim), mean=0., stddev=self.epsilon_std)\n",
    "        output = z_mean_transformed + K.exp(0.5 * z_log_var_transformed) * epsilon\n",
    "        #print('Sampling output shape:', output.shape)\n",
    "        return output\n",
    "\n",
    "    def _build(self):\n",
    "        \"\"\"\n",
    "        Build the VAE model.\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        inputs = Input(shape=(self.sequence_length, self.original_dim), name='encoder_input')\n",
    "        x = LSTM(self.intermediate_dim, activation='relu', return_sequences=True)(inputs)\n",
    "        x = SelfAttentionLayer(num_heads=4, key_dim=self.intermediate_dim)(x)  # Self-Attention layer\n",
    "        x = LSTM(self.intermediate_dim, activation='relu', return_sequences=False)(x)\n",
    "\n",
    "        z_mean = Dense(self.latent_dim, name='z_mean')(x)\n",
    "        z_log_var = Dense(self.latent_dim, name='z_log_var')(x)\n",
    "        #print('z_mean shape:', z_mean.shape)\n",
    "        #print('z_log_var shape:', z_log_var.shape)\n",
    "\n",
    "        z = Lambda(self._sampling, output_shape=(self.latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "        # Instantiate the encoder model\n",
    "        self.encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "        \n",
    "        # Decoder\n",
    "        latent_inputs = Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "        x = RepeatVector(self.sequence_length)(latent_inputs)\n",
    "        x = LSTM(self.intermediate_dim, activation='relu', return_sequences=True)(x)\n",
    "        x = LSTM(self.original_dim, activation='relu', return_sequences=True)(x)\n",
    "        #print('Decoder output shape before final Dense layer:', x.shape)\n",
    "\n",
    "        outputs = TimeDistributed(Dense(self.original_dim))(x)\n",
    "\n",
    "        # Instantiate the decoder model\n",
    "        self.decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "        # VAE model\n",
    "        outputs = self.decoder(self.encoder(inputs)[2])\n",
    "        self.vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "#     def vae_loss(self, x, x_decoded_mean, z_mean, z_log_var):\n",
    "#         print('x:', x)\n",
    "#         print('x_decoded_mean:', x_decoded_mean)\n",
    "#         mse = tf.reduce_mean(tf.square(x - x_decoded_mean))\n",
    "#         print('mse:', mse)\n",
    "#         xent_loss = self.sequence_length * mse\n",
    "#         print('xent_loss:', xent_loss)\n",
    "#         kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "#         print('kl_loss:', kl_loss)\n",
    "#         print('K.mean(xent_loss + kl_loss):', K.mean(xent_loss + kl_loss))\n",
    "#         return K.mean(xent_loss + kl_loss)\n",
    "    def vae_loss(self, x, x_decoded_mean, z_mean, z_log_var):\n",
    "        mse = tf.reduce_mean(tf.square(x - x_decoded_mean), axis=(1, 2))\n",
    "        xent_loss = mse\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    \n",
    "    def compile(self, optimizer='adam'):\n",
    "        # Define a wrapper function for the VAE loss\n",
    "        def vae_loss_wrapper(x, x_decoded_mean):\n",
    "            z_mean, z_log_var, _ = self.encoder(x)\n",
    "            return self.vae_loss(x, x_decoded_mean, z_mean, z_log_var)\n",
    "\n",
    "        self.vae.compile(optimizer=optimizer, loss=vae_loss_wrapper)\n",
    "\n",
    "#     def vae_loss(self, x, x_decoded_mean):\n",
    "#         print('x:', x)\n",
    "#         print('x_decoded_mean:', x_decoded_mean)\n",
    "#         mse = tf.reduce_mean(tf.square(x - x_decoded_mean))\n",
    "#         print('mse:', mse)\n",
    "#         xent_loss = self.sequence_length * mse\n",
    "#         print('xent_loss:', xent_loss)\n",
    "#         kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "#         print('kl_loss:', kl_loss)\n",
    "#         print('K.mean(xent_loss + kl_loss):', K.mean(xent_loss + kl_loss))\n",
    "#         return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "\n",
    "# Instantiate and Compile the VAE\n",
    "sequence_length = 10\n",
    "original_dim = 2\n",
    "intermediate_dim = 50\n",
    "latent_dim = 25\n",
    "\n",
    "vae_model = VAE(sequence_length, original_dim, intermediate_dim, latent_dim)\n",
    "#vae_model.vae.compile(optimizer='adam', loss=vae_model.vae_loss)\n",
    "vae_model.compile()\n",
    "\n",
    "# Model Training\n",
    "batch_size = 20\n",
    "max_train_samples = 500000\n",
    "train_steps = max_train_samples // (batch_size * sequence_length)\n",
    "num_epochs = 4\n",
    "max_samples = 500000  # Maximum samples to read (or None to read all)\n",
    "max_test_samples = 500000\n",
    "\n",
    "#pure_file_pattern = '/home/mreza/5G accelerator/ID_MEC/data generator/pure_data/pure_iq_samples_*.csv'\n",
    "#mixed_file_pattern = '/home/mreza/5G accelerator/ID_MEC/data generator/mixed_data/mixed_iq_samples_*.csv'\n",
    "# Example file patterns\n",
    "pure_file_pattern = 'C:\\\\Users\\\\Mohammadreza\\\\Desktop\\\\My Class\\\\Proj-DC\\\\My Works\\\\My Papers\\\\intrusion\\\\data generator\\\\pure_data\\\\pure_iq_samples_*.csv'\n",
    "mixed_file_pattern = 'C:\\\\Users\\\\Mohammadreza\\\\Desktop\\\\My Class\\\\Proj-DC\\\\My Works\\\\My Papers\\\\intrusion\\\\data generator\\\\mixed_data\\\\mixed_iq_samples_*.csv'\n",
    "\n",
    "# Data Generator Instances\n",
    "train_gen_instance = CSVDataGenerator(pure_file_pattern, batch_size, sequence_length, \n",
    "                                      max_train_samples, for_training=True)\n",
    "combined_gen_instance = CSVDataGenerator(mixed_file_pattern, batch_size, sequence_length, \n",
    "                                         max_test_samples, for_training=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_gen_instance.__iter__()  # Reset the generator at the beginning of each epoch\n",
    "\n",
    "    for step in range(train_steps):\n",
    "        #print('step in train loop:', step)\n",
    "        try:\n",
    "            #print('Im in try of training loop')\n",
    "            X_chunk, Y_chunk = next(train_gen_instance)\n",
    "            #print('X_chunk.shape:', X_chunk.shape)\n",
    "            #print('Y_chunk.shape:', Y_chunk.shape)\n",
    "        except StopIteration:\n",
    "            #print('Im in except of training loop')\n",
    "            # Reset the generator if needed\n",
    "            train_gen_instance.__iter__()\n",
    "            X_chunk, Y_chunk = next(train_gen_instance)\n",
    "            #print('X_chunk.shape:', X_chunk.shape)\n",
    "            #print('Y_chunk.shape:', Y_chunk.shape)\n",
    "\n",
    "        #print('inside for step loop just before loss')\n",
    "        # Train the VAE model on the batch and get the loss value\n",
    "        loss = vae_model.vae.train_on_batch(X_chunk, Y_chunk)\n",
    "\n",
    "        # Print the loss every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step + 1}/{train_steps}, Loss: {loss}\")\n",
    "\n",
    "            # Check for NaN in model predictions\n",
    "            pred_sample = vae_model.vae.predict(X_chunk[:1])\n",
    "            if np.isnan(pred_sample).any():\n",
    "                print(\"NaN detected in model predictions during training.\")\n",
    "                break  # Optional: break the loop if NaN is detected\n",
    "\n",
    "    print()  # Newline for better readability\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prediction Phase Setup\n",
    "num_predictions = 10\n",
    "print(f\"Number of predictions to be performed: {num_predictions}\")\n",
    "\n",
    "combined_gen_instance.reset_for_prediction()\n",
    "\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "reconstruction_errors = []\n",
    "all_X_chunk_test = []\n",
    "all_X_chunk_pred = []\n",
    "all_intrusion_flags = []\n",
    "accuracies = []\n",
    "\n",
    "default_threshold = 0.5  # Define a default threshold\n",
    "last_valid_threshold = default_threshold  # Initialize the last valid threshold\n",
    "\n",
    "# Prediction Loop\n",
    "try:    \n",
    "    for i in range(num_predictions):\n",
    "        print(f'Prediction number: {i}')\n",
    "        X_chunk_test, current_labels = next(combined_gen_instance)\n",
    "\n",
    "        # Ensure X_chunk_test shape is correct\n",
    "        if X_chunk_test.shape[1] != sequence_length or X_chunk_test.shape[2] != 2:\n",
    "            raise ValueError(f\"Incorrect shape for X_chunk_test: {X_chunk_test.shape}\")\n",
    "\n",
    "        X_chunk_pred = vae_model.vae.predict(X_chunk_test)\n",
    "\n",
    "        # Compute errors\n",
    "        chunk_errors = np.mean(np.square(X_chunk_test - X_chunk_pred), axis=1)\n",
    "        max_error_per_sequence = chunk_errors.max(axis=1)\n",
    "\n",
    "        if max_error_per_sequence.size == 0:\n",
    "            print(\"max_error_per_sequence is empty, skipping this batch.\")\n",
    "            continue\n",
    "\n",
    "        # Determine the actual number of sequences\n",
    "        num_sequences = X_chunk_test.shape[0]\n",
    "        error_per_sequence = max_error_per_sequence.reshape(num_sequences, -1).mean(axis=1)\n",
    "\n",
    "        # Handle NaN values in error_per_sequence for threshold calculation\n",
    "        if not np.isnan(error_per_sequence).any():\n",
    "            threshold1 = np.nanpercentile(error_per_sequence, 95)\n",
    "            last_valid_threshold = threshold1  # Update last valid threshold\n",
    "        else:\n",
    "            threshold1 = last_valid_threshold\n",
    "\n",
    "        intrusion_detected_inloop = error_per_sequence > threshold1\n",
    "        batch_accuracy = accuracy_score(current_labels[:len(error_per_sequence)], intrusion_detected_inloop)\n",
    "        accuracies.append(batch_accuracy)\n",
    "\n",
    "        # Append to respective lists\n",
    "        true_labels.extend(current_labels[:len(error_per_sequence)])\n",
    "        predicted_labels.extend(intrusion_detected_inloop)\n",
    "        reconstruction_errors.extend(chunk_errors)\n",
    "        all_X_chunk_test.append(X_chunk_test)\n",
    "        all_X_chunk_pred.append(X_chunk_pred)\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"All samples processed.\")\n",
    "\n",
    "reconstruction_error = np.array(reconstruction_errors)\n",
    "print('reconstruction_error.shape:', reconstruction_error.shape)\n",
    "max_error_per_sequence = reconstruction_error.max(axis=1)\n",
    "print('max_error_per_sequence.shape:', max_error_per_sequence.shape)\n",
    "threshold1 = np.percentile(reconstruction_error, 95)\n",
    "print('threshold1:', threshold1)\n",
    "threshold2 = np.percentile(reconstruction_error, 95)\n",
    "print('threshold2:', threshold2)\n",
    "\n",
    "is_intrusion_detected = error_per_sequence > threshold1\n",
    "\n",
    "# ... remaining code for analysis ...\n",
    "\n",
    "    \n",
    "\n",
    "# reconstruction_error = np.array(reconstruction_errors)\n",
    "# print('reconstruction_error.shape:', reconstruction_error.shape)\n",
    "# max_error_per_sequence = reconstruction_error.max(axis=1)\n",
    "# print('max_error_per_sequence.shape:', max_error_per_sequence.shape)\n",
    "# threshold1 = np.percentile(max_error_per_sequence, 95)\n",
    "# print('threshold1:', threshold1)\n",
    "# threshold2 = np.percentile(reconstruction_error, 95)\n",
    "# print('threshold percentile:', threshold2)\n",
    "\n",
    "# is_intrusion_detected = error_per_sequence > threshold1  # Boolean array for sequences, shape (num_predictions * batch_size,)\n",
    "num_total_sequences = num_predictions * batch_size - num_predictions\n",
    "print('len(is_intrusion_detected):', len(is_intrusion_detected))\n",
    "print('num_total_sequences:', num_total_sequences)\n",
    "#---------------------------------------finish 111-----------------------------------\n",
    "flat_error_per_sequence = error_per_sequence.flatten()\n",
    "# Determine if intrusion detected for each sequence\n",
    "for error in flat_error_per_sequence:\n",
    "    all_intrusion_flags.append(error > threshold1)    \n",
    "all_X_chunk_test = np.concatenate(all_X_chunk_test, axis=0)\n",
    "all_X_chunk_pred = np.concatenate(all_X_chunk_pred, axis=0)\n",
    "save_path = 'C:\\\\Users\\\\Mohammadreza\\\\Desktop\\\\My Class\\\\Proj-DC\\\\My Works\\\\My Papers\\\\intrusion\\\\data generator\\\\intrusion_detected'\n",
    "#plot_with_intrusions8(all_X_chunk_test, all_X_chunk_pred, all_intrusion_flags, sequence_length, save_path)\n",
    "\n",
    "jamming_detected = reconstruction_error > threshold1\n",
    "train_gen_instance.close()\n",
    "combined_gen_instance.close()\n",
    "#Table\n",
    "flattened_jamming_detected = jamming_detected.flatten()\n",
    "real_part_detected = jamming_detected[:, 0]\n",
    "imag_part_detected = jamming_detected[:, 1]\n",
    "\n",
    "real_true_count = np.sum(real_part_detected)\n",
    "real_false_count = len(real_part_detected) - real_true_count\n",
    "\n",
    "imag_true_count = np.sum(imag_part_detected)\n",
    "imag_false_count = len(imag_part_detected) - imag_true_count\n",
    "# Overall\n",
    "overall_true_count = np.sum(flattened_jamming_detected)\n",
    "overall_false_count = len(flattened_jamming_detected) - overall_true_count\n",
    "# Table-DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Part': ['Real', 'Imaginary', 'Overall'],\n",
    "    'True Count': [real_true_count, imag_true_count, overall_true_count],\n",
    "    'False Count': [real_false_count, imag_false_count, overall_false_count]\n",
    "})\n",
    "print(df)\n",
    "num_jamming_detected = np.sum(jamming_detected)\n",
    "print(f\"Number of jamming sequences detected: {num_jamming_detected} out of {len(flattened_jamming_detected)} sequences\")\n",
    "\n",
    "\n",
    "true_labels = np.array(true_labels).flatten()\n",
    "predicted_labels = np.array(predicted_labels, dtype=int).flatten()\n",
    "#predicted_labels = np.array(reconstruction_errors > threshold2, dtype=int).flatten()\n",
    "\n",
    "print(\"Length of true labels:\", len(true_labels))\n",
    "print(\"Length of predicted labels:\", len(predicted_labels))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "try:\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    roc_auc = roc_auc_score(true_labels, predicted_labels)\n",
    "    fpr, tpr, _ = roc_curve(true_labels, predicted_labels)\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(f\"ROC AUC: {roc_auc}\")\n",
    "    print(report)\n",
    "\n",
    "    # Plot ROC Curve\n",
    "#     plt.figure()\n",
    "#     plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc})')\n",
    "#     plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('Receiver Operating Characteristic')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Error in calculating metrics:\", e)\n",
    "\n",
    "#accuracy\n",
    "plt.plot(accuracies)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6)) \n",
    "plt.plot(accuracies, linewidth=2)\n",
    "plt.title('Accuracy Prediction', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Batch Number', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=14, fontweight='bold')\n",
    "# for font size of x-axis and y-axis values\n",
    "for label in (plt.gca().get_xticklabels() + plt.gca().get_yticklabels()):\n",
    "    label.set_fontsize(12)\n",
    "    label.set_fontweight('bold')\n",
    "plt.ylim(0, 1)  # Set y-axis limit to 0 to 1\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "accuracies_series = pd.Series(accuracies)\n",
    "\n",
    "# Smoothing-calculate the rolling mean (moving average) in window size\n",
    "window_size = 10  # adjust to change the smoothing level\n",
    "smoothed_accuracies = accuracies_series.rolling(window=window_size).mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(accuracies, linewidth=2, alpha=0.4, label='Original')  #alpha for transparency\n",
    "plt.plot(smoothed_accuracies, linewidth=2, color='red', label='Smoothed') # color to red\n",
    "plt.title('Accuracy Prediction', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Batch Number', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=14, fontweight='bold')\n",
    "for label in (plt.gca().get_xticklabels() + plt.gca().get_yticklabels()):\n",
    "    label.set_fontsize(12)\n",
    "    label.set_fontweight('bold')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reconstruction error\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(reconstruction_error, label='Reconstruction Error')\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error with Threshold')\n",
    "plt.xlabel('Sequence Number')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('1-Reconstruction Error with Threshold.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "# reconstruction error\n",
    "reconstruction_error_real = reconstruction_error[:, 0]\n",
    "reconstruction_error_imag = reconstruction_error[:, 1]\n",
    "\n",
    "# Plot for Real Part\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(reconstruction_error_real, label='Reconstruction Error - Real Part', color='blue')\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error for Real Part with Threshold')\n",
    "plt.xlabel('Sequence Number')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('2-Reconstruction Error for Real Part with Threshold.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "# Plot for Imaginary Part\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(reconstruction_error_imag, label='Reconstruction Error - Imaginary Part', color='orange')\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error for Imaginary Part with Threshold')\n",
    "plt.xlabel('Sequence Number')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('3-Reconstruction Error for Imaginary Part with Threshold.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Histogram of Reconstruction Errors:\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.hist(reconstruction_error, bins=50, alpha=0.75)\n",
    "plt.axvline(x=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Histogram of Reconstruction Errors')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "# plt.savefig('4-Histogram of Reconstruction Errors.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Time Series Plot of IQ Samples:\n",
    "sample_index = np.random.choice(len(X_chunk_test))\n",
    "original_sample = X_chunk_test[sample_index]\n",
    "reconstructed_sample = X_chunk_pred[sample_index]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title('Original vs Reconstructed IQ Data')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('5-Original vs Reconstructed IQ Data.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "#Scatter Plot of Reconstruction Errors vs. Real and Imaginary Parts:\n",
    "avg_real = np.mean(X_chunk_test, axis=1)[:, 0]\n",
    "avg_imag = np.mean(X_chunk_test, axis=1)[:, 1]\n",
    "\n",
    "last_errors = np.mean(reconstruction_errors[-len(X_chunk_test):], axis=1)\n",
    "\n",
    "print(\"Shape of avg_real:\", avg_real.shape)\n",
    "print(\"Shape of avg_imag:\", avg_imag.shape)\n",
    "print(\"Shape of last_errors:\", len(last_errors))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(avg_real, last_errors, label='Real Part', alpha=0.5)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error vs. Average Real Part')\n",
    "plt.xlabel('Average Amplitude')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('6-Reconstruction Error vs. Average Real Part.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(avg_imag, last_errors, label='Imaginary Part', alpha=0.5)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error vs. Average Imaginary Part')\n",
    "plt.xlabel('Average Amplitude')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('7-Reconstruction Error vs. Average Imaginary Part.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "# # Define the number of sequences to plot together\n",
    "n = 1  # Change this to desired number of sequences\n",
    "sample_length = sequence_length * n\n",
    "\n",
    "# Select a random starting sequence for plotting\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "\n",
    "# Extract and concatenate the original and reconstructed samples\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "# Plot concatenated sequences\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('9-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for n = 9\n",
    "n = 1  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction error\n",
    "reconstruction_error_real = reconstruction_error[:, 0]\n",
    "reconstruction_error_imag = reconstruction_error[:, 1]\n",
    "\n",
    "# Plot for Real Part\n",
    "plt.figure(figsize=(14, 6))\n",
    "mellow_green = '#89C997' \n",
    "plt.plot(reconstruction_error_real, label='Reconstruction Error', color=mellow_green)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Intrusion Detected by Reconstruction Error',fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Sequence Number (×10³)', fontsize=16, fontweight='bold')\n",
    "#plt.xlabel('Sequence Number(*1000)', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Reconstruction Error', fontsize=16, fontweight='bold')\n",
    "for label in (plt.gca().get_xticklabels() + plt.gca().get_yticklabels()):\n",
    "    label.set_fontsize(12)\n",
    "    label.set_fontweight('bold')\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce77350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "plt.plot(accuracies)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(accuracies, linewidth=2)\n",
    "plt.title('Accuracy Prediction', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Batch Number', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=14, fontweight='bold')\n",
    "# set font size x-axis and y-axis values\n",
    "for label in (plt.gca().get_xticklabels() + plt.gca().get_yticklabels()):\n",
    "    label.set_fontsize(12)\n",
    "    label.set_fontweight('bold')\n",
    "plt.ylim(0, 1)  # Set y-axis limit to range to 0 to 1\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_series = pd.Series(accuracies)\n",
    "\n",
    "# Smoothing-calculate the rolling mean (moving average) in window size\n",
    "window_size = 10  # adjust to change the smoothing level\n",
    "smoothed_accuracies = accuracies_series.rolling(window=window_size).mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(accuracies, linewidth=2, alpha=0.4, label='Original')  #alpha for transparency\n",
    "plt.plot(smoothed_accuracies, linewidth=2, color='red', label='Smoothed') # color to red\n",
    "plt.title('Accuracy Prediction', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Batch Number', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=14, fontweight='bold')\n",
    "for label in (plt.gca().get_xticklabels() + plt.gca().get_yticklabels()):\n",
    "    label.set_fontsize(12)\n",
    "    label.set_fontweight('bold')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42b73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc4676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
