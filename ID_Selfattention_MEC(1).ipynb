{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d3f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Processing file: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\My Papers\\intrusion\\data generator\\pure_data\\pure_iq_samples_1.csv\n",
      "Processing file: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\My Papers\\intrusion\\data generator\\pure_data\\pure_iq_samples_2.csv\n",
      "Processing file: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\My Papers\\intrusion\\data generator\\pure_data\\pure_iq_samples_3.csv\n",
      "Step 10000/10000\n",
      "Epoch 2/5\n",
      "Processing file: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\My Papers\\intrusion\\data generator\\pure_data\\pure_iq_samples_1.csv\n",
      "Processing file: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\My Papers\\intrusion\\data generator\\pure_data\\pure_iq_samples_2.csv\n",
      "Step 9755/10000\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Layer\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import struct\n",
    "import glob\n",
    "\n",
    "# MEC related functions\n",
    "def mec_kocaoglu_np(p, q):\n",
    "    p = p.copy().astype(np.float64) / p.sum()\n",
    "    q = q.copy().astype(np.float64) / q.sum()\n",
    "    J = np.zeros((len(q), len(p)), dtype=np.float64)\n",
    "    M = np.stack((p, q), 0)\n",
    "    r = M.max(axis=1).min()\n",
    "    while r > 0:\n",
    "        a_i = M.argmax(axis=1)\n",
    "        M[0, a_i[0]] -= r\n",
    "        M[1, a_i[1]] -= r\n",
    "        J[a_i[0], a_i[1]] = r\n",
    "        r = M.max(axis=1).min()\n",
    "    return J\n",
    "\n",
    "def apply_mec_to_data(data):\n",
    "    data_distribution = np.histogram(data, bins='auto')[0].astype(np.float64)\n",
    "    data_distribution /= data_distribution.sum()\n",
    "    mec_transformed = mec_kocaoglu_np(data_distribution, data_distribution)\n",
    "    return mec_transformed.sum(0)\n",
    "\n",
    "# Custom loss function\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    entropy_reg = -tf.reduce_mean(y_pred * tf.math.log(y_pred + 1e-9))\n",
    "    lambda_entropy = 0.01\n",
    "    return mse + lambda_entropy * entropy_reg\n",
    "\n",
    "# Data Generator\n",
    "class CSVDataGenerator:\n",
    "    def __init__(self, file_pattern, batch_size, sequence_length, max_samples=None, for_training=True):     \n",
    "        self.file_pattern = file_pattern\n",
    "        self.file_list = sorted(glob.glob(self.file_pattern))\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_samples = max_samples\n",
    "        self.for_training = for_training\n",
    "        self.current_file_idx = 0\n",
    "        self.dataframe_iterator = None\n",
    "        self.labels_iterator = None\n",
    "        self.samples_buffer = []\n",
    "        self.labels_buffer = []\n",
    "        self.total_samples_processed = 0\n",
    "\n",
    "    def _load_next_file(self):\n",
    "        if self.current_file_idx >= len(self.file_list):\n",
    "            print(\"No more files to process.\")\n",
    "            raise StopIteration\n",
    "\n",
    "        current_file = self.file_list[self.current_file_idx]\n",
    "        df = pd.read_csv(current_file)\n",
    "        # Filter out rows where 'IQ Data' is '0j'\n",
    "        df['IQ Data'] = df['IQ Data'].apply(lambda x: complex(x.replace('i', 'j')))\n",
    "        df = df[df['IQ Data'] != 0j]\n",
    "\n",
    "        # Check if DataFrame is empty or 'IQ Data' column is missing\n",
    "        if df.empty or 'IQ Data' not in df.columns:\n",
    "            raise ValueError(f\"File {current_file} is empty or missing 'IQ Data' column after filtering 0j.\")\n",
    "        # If not for training, extract the labels\n",
    "        if not self.for_training and 'label' in df.columns:\n",
    "            self.labels_iterator = iter(df['label'].map(lambda x: 1 if x == 'jammer' else 0).values)\n",
    "        else:\n",
    "            self.labels_iterator = None\n",
    "        print(f\"Processing file: {current_file}\")\n",
    "        self.dataframe_iterator = iter(df['IQ Data'].values)\n",
    "        self.current_file_idx += 1\n",
    "\n",
    "    # Reset function to be used when switching from training to prediction\n",
    "    def reset_for_prediction(self):\n",
    "        self.current_file_idx = 0\n",
    "        self.samples_buffer = []\n",
    "        self.labels_buffer = []\n",
    "        self.total_samples_processed = 0\n",
    "        self.dataframe_iterator = None\n",
    "        self._load_next_file()  # Start from the first file again\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_file_idx = 0\n",
    "        self.samples_buffer = []\n",
    "        self.labels_buffer = []\n",
    "        self.total_samples_processed = 0\n",
    "        self._load_next_file()\n",
    "        return self\n",
    "    \n",
    "    def process_data(self, samples):\n",
    "        real_parts = np.real(samples)\n",
    "        imag_parts = np.imag(samples)\n",
    "\n",
    "        # Normalization\n",
    "        epsilon = 1e-9\n",
    "        real_parts_normalized = (real_parts - np.mean(real_parts)) / (np.std(real_parts) + epsilon)\n",
    "        imag_parts_normalized = (imag_parts - np.mean(imag_parts)) / (np.std(imag_parts) + epsilon)\n",
    "        #print(f\"Normalized real_parts: {real_parts_normalized.shape}\")\n",
    "        #print(f\"Normalized imag_parts: {imag_parts_normalized.shape}\")\n",
    "\n",
    "        # NaN handling\n",
    "        real_parts_normalized = np.nan_to_num(real_parts_normalized)\n",
    "        imag_parts_normalized = np.nan_to_num(imag_parts_normalized)\n",
    "\n",
    "        # Apply MEC transformation\n",
    "        transformed_real = apply_mec_to_data(real_parts_normalized)\n",
    "        transformed_imag = apply_mec_to_data(imag_parts_normalized)\n",
    "        #print(f\"Transformed transformed_real: {transformed_real.shape}\")\n",
    "        #print(f\"Transformed transformed_imag: {transformed_imag.shape}\")\n",
    "\n",
    "        # Ensure transformed_real and transformed_imag are of the same length\n",
    "        max_length = max(transformed_real.shape[0], transformed_imag.shape[0])\n",
    "        transformed_real = np.pad(transformed_real, (0, max_length - transformed_real.shape[0]), mode='constant')\n",
    "        transformed_imag = np.pad(transformed_imag, (0, max_length - transformed_imag.shape[0]), mode='constant')\n",
    "        #print(f\"Rescaled transformed_real: {transformed_real.shape}\")\n",
    "        #print(f\"Rescaled transformed_imag: {transformed_imag.shape}\")\n",
    "\n",
    "        # Combine real and imaginary parts\n",
    "        combined = np.concatenate([transformed_real.reshape(-1, 1), transformed_imag.reshape(-1, 1)], axis=-1)\n",
    "        #print(f\"Combined data shape before reshaping: {combined.shape}\")\n",
    "\n",
    "        # Reshape for the RNN autoencoder\n",
    "        if combined.shape[0] < self.batch_size * self.sequence_length:\n",
    "            #print('Padding too short sequence')\n",
    "            combined = np.pad(combined, ((0, self.batch_size * self.sequence_length - combined.shape[0]), (0, 0)), mode='constant')\n",
    "        elif combined.shape[0] > self.batch_size * self.sequence_length:\n",
    "            #print('Truncating too long sequence')\n",
    "            combined = combined[:self.batch_size * self.sequence_length]\n",
    "\n",
    "        combined = combined.reshape(-1, self.sequence_length, 2)\n",
    "        #print(f\"Final reshaped data: {combined.shape}\")\n",
    "\n",
    "        return combined\n",
    "\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.max_samples and self.total_samples_processed >= self.max_samples:\n",
    "            raise StopIteration(\"Reached max_samples limit.\")\n",
    "\n",
    "        while len(self.samples_buffer) < self.batch_size * self.sequence_length:\n",
    "            try:\n",
    "                chunk = next(self.dataframe_iterator)\n",
    "                self.samples_buffer.append(chunk)\n",
    "                if not self.for_training and self.labels_iterator is not None:\n",
    "                    label_chunk = next(self.labels_iterator)\n",
    "                    self.labels_buffer.append(label_chunk)\n",
    "            except StopIteration:\n",
    "                if self.current_file_idx >= len(self.file_list):\n",
    "                    raise StopIteration(\"No more data to process.\")\n",
    "                self._load_next_file()\n",
    "        #print(f\"Buffer Size Before Slicing: {len(self.samples_buffer)}\")\n",
    "        samples = self.samples_buffer[:self.batch_size * self.sequence_length]\n",
    "        self.samples_buffer = self.samples_buffer[self.batch_size * self.sequence_length:]\n",
    "        #print(f\"Buffer Size After Slicing: {len(self.samples_buffer)}\")\n",
    "        #print(f\"Sample Size Before Processing: {len(samples)}\")\n",
    "\n",
    "        X_chunk = self.process_data(np.array(samples))\n",
    "        \n",
    "        if X_chunk is None:\n",
    "            raise ValueError(\"Incorrect chunk size, unable to reshape.\")\n",
    "\n",
    "        if not self.for_training:\n",
    "            labels = self.labels_buffer[:self.batch_size * self.sequence_length]\n",
    "            self.labels_buffer = self.labels_buffer[self.batch_size * self.sequence_length:]\n",
    "            return X_chunk, np.array(labels)\n",
    "        else:\n",
    "            return X_chunk, X_chunk\n",
    "        \n",
    "    def close(self):\n",
    "        self.samples_buffer = []\n",
    "        self.labels_buffer = []\n",
    "        self.total_samples_processed = 0\n",
    "        self.current_file_idx = 0\n",
    "        self.dataframe_iterator = None\n",
    "        self.labels_iterator = None   \n",
    "# LSTM Autoencoder Model\n",
    "class SelfAttentionLayer(Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.multi_head_attention(inputs, inputs, inputs)\n",
    "\n",
    "sequence_length = 10\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(sequence_length, 2), return_sequences=True))\n",
    "model.add(SelfAttentionLayer(num_heads=4, key_dim=50))\n",
    "model.add(LSTM(25, activation='relu', return_sequences=False))\n",
    "model.add(RepeatVector(sequence_length))\n",
    "model.add(LSTM(25, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(2)))\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss)\n",
    "\n",
    "# Model Training\n",
    "batch_size = 20\n",
    "max_train_samples = 2000000\n",
    "train_steps = max_train_samples // (batch_size * sequence_length)\n",
    "max_samples = 2000000  # Maximum samples to read (or None to read all)\n",
    "max_test_samples = 2000000\n",
    "pure_file_pattern = 'C:\\\\Users\\\\Mohammadreza\\\\Desktop\\\\My Class\\\\Proj-DC\\\\My Works\\\\My Papers\\\\intrusion\\\\data generator\\\\pure_data\\\\pure_iq_samples_*.csv'\n",
    "mixed_file_pattern = 'C:\\\\Users\\\\Mohammadreza\\\\Desktop\\\\My Class\\\\Proj-DC\\\\My Works\\\\My Papers\\\\intrusion\\\\data generator\\\\mixed_data\\\\mixed_iq_samples_*.csv'\n",
    "num_epochs = 5\n",
    "steps_per_epoch = train_steps\n",
    "\n",
    "train_gen_instance = CSVDataGenerator(pure_file_pattern, batch_size, sequence_length, \n",
    "                                      max_train_samples, for_training=True)\n",
    "combined_gen_instance = CSVDataGenerator(mixed_file_pattern, batch_size, sequence_length, \n",
    "                                         max_test_samples, for_training=False)\n",
    "# combined_gen_instance = CSVDataGenerator(mixed_file_pattern, batch_size, sequence_length, \n",
    "#                                          max_train_samples, for_training=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_gen_instance.__iter__() # Reset the generator at the beginning of each epoch\n",
    "    for step in range(steps_per_epoch):\n",
    "        try:\n",
    "            X_chunk, Y_chunk = next(train_gen_instance)\n",
    "            X_chunk_transformed = train_gen_instance.process_data(X_chunk)\n",
    "            model.train_on_batch(X_chunk_transformed, Y_chunk)\n",
    "            print(f\"Step {step + 1}/{steps_per_epoch}\", end='\\r')\n",
    "        except StopIteration:\n",
    "            train_gen_instance.__iter__()\n",
    "            X_chunk, Y_chunk = next(train_gen_instance)\n",
    "    print()\n",
    "    \n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     train_gen_instance.__iter__() # Reset the generator at the beginning of each epoch\n",
    "#     for step in range(steps_per_epoch):\n",
    "#         try:\n",
    "#             X_chunk, Y_chunk = next(train_gen_instance)\n",
    "#             X_chunk_transformed = train_gen_instance.process_data(X_chunk)\n",
    "#         except StopIteration:\n",
    "#             train_gen_instance.__iter__()\n",
    "#             X_chunk, Y_chunk = next(train_gen_instance)\n",
    "#         model.train_on_batch(X_chunk_transformed, Y_chunk)\n",
    "#         print(f\"Step {step + 1}/{steps_per_epoch}\", end='\\r')\n",
    "#     print()\n",
    "\n",
    "num_predictions = 500  # or any other large number\n",
    "print(f\"Number of predictions to be performed: {num_predictions}\")\n",
    "\n",
    "combined_gen_instance.reset_for_prediction()\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "reconstruction_errors = []\n",
    "all_X_chunk_test = []\n",
    "all_X_chunk_test_transformed = []\n",
    "all_X_chunk_pred = []\n",
    "all_intrusion_flags = []\n",
    "\n",
    "# Prediction Phase\n",
    "num_predictions = 10\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "reconstruction_errors = []\n",
    "\n",
    "combined_gen_instance.reset_for_prediction()\n",
    "# for i in range(num_predictions):\n",
    "#     try:\n",
    "#         X_chunk_test, current_labels = next(combined_gen_instance)\n",
    "#         X_chunk_test_transformed = combined_gen_instance.process_data(X_chunk_test)\n",
    "#         X_chunk_pred = model.predict(X_chunk_test_transformed)\n",
    "#         # ... [rest of your prediction logic] ...\n",
    "#     except StopIteration:\n",
    "#         print(\"All samples processed.\")\n",
    "#         break\n",
    "\n",
    "# # ... [rest of your code for calculating thresholds, errors, and predictions] ...\n",
    "\n",
    "try:    \n",
    "    for i in range(num_predictions):\n",
    "        print(f'Prediction number: {i}')\n",
    "        X_chunk_test, current_labels = next(combined_gen_instance)\n",
    "        #print(f'Shape of X_chunk_test: {X_chunk_test.shape}')\n",
    "        #X_chunk_test_transformed = combined_gen_instance.process_data(X_chunk_test)\n",
    "        X_chunk_test = combined_gen_instance.process_data(X_chunk_test)\n",
    "        #print(f'Shape of X_chunk_test after transform: {X_chunk_test.shape}')\n",
    "\n",
    "        X_chunk_pred = model.predict(X_chunk_test)\n",
    "        #X_chunk_pred = model.predict(X_chunk_test_transformed)\n",
    "        #print(f'Shape of X_chunk_pred: {X_chunk_pred.shape}')\n",
    "\n",
    "        chunk_errors = np.mean(np.square(X_chunk_test - X_chunk_pred), axis=1)\n",
    "        #chunk_errors = np.mean(np.square(X_chunk_test_transformed - X_chunk_pred), axis=1)\n",
    "        #print(f'Shape of chunk_errors: {chunk_errors.shape}')\n",
    "\n",
    "        max_error_per_sequence = chunk_errors.max(axis=1)\n",
    "        #print(f'Size of max_error_per_sequence: {max_error_per_sequence.size}')\n",
    "\n",
    "        # Check if max_error_per_sequence is empty\n",
    "        if max_error_per_sequence.size == 0:\n",
    "            print(\"max_error_per_sequence is empty, skipping this batch.\")\n",
    "            continue\n",
    "\n",
    "        error_per_sequence = max_error_per_sequence.reshape(-1, sequence_length).mean(axis=1)\n",
    "\n",
    "        if error_per_sequence.size > 0:\n",
    "            threshold1 = np.percentile(error_per_sequence, 95)\n",
    "            print(f'Threshold1: {threshold1}')\n",
    "        else:\n",
    "            print(\"error_per_sequence is empty, skipping percentile calculation.\")\n",
    "            continue\n",
    "        intrusion_detected_inloop = error_per_sequence > threshold1\n",
    "\n",
    "        # Append to respective lists\n",
    "        true_labels.extend(current_labels[:len(error_per_sequence)])       \n",
    "        predicted_labels.extend(intrusion_detected_inloop)\n",
    "        reconstruction_errors.extend(chunk_errors)\n",
    "        all_X_chunk_test.append(X_chunk_test)\n",
    "        #X_chunk_test_transformed.append(X_chunk_test_transformed)\n",
    "        all_X_chunk_pred.append(X_chunk_pred)\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"All samples processed.\")\n",
    "\n",
    "    \n",
    "reconstruction_error = np.array(reconstruction_errors)\n",
    "\n",
    "max_error_per_sequence = reconstruction_error.reshape(-1, 2).max(axis=1)  # Shape (num_predictions * batch_size * sequence_length,)\n",
    "error_per_sequence = max_error_per_sequence.reshape(-1, sequence_length).mean(axis=1)  # Shape (num_predictions * batch_size,)\n",
    "threshold1 = np.percentile(error_per_sequence, 95)\n",
    "print('threshold1:', threshold1)\n",
    "threshold2 = np.percentile(reconstruction_error, 95)\n",
    "print('threshold percentile:', threshold2)\n",
    "\n",
    "is_intrusion_detected = error_per_sequence > threshold1  # Boolean array for sequences, shape (num_predictions * batch_size,)\n",
    "num_total_sequences = num_predictions * batch_size - num_predictions\n",
    "print('len(is_intrusion_detected):', len(is_intrusion_detected))\n",
    "print('num_total_sequences:', num_total_sequences)\n",
    "#---------------------------------------finish 111-----------------------------------\n",
    "flat_error_per_sequence = error_per_sequence.flatten()\n",
    "# Determine if intrusion detected for each sequence\n",
    "for error in flat_error_per_sequence:\n",
    "    all_intrusion_flags.append(error > threshold1)    \n",
    "all_X_chunk_test = np.concatenate(all_X_chunk_test, axis=0)\n",
    "all_X_chunk_pred = np.concatenate(all_X_chunk_pred, axis=0)\n",
    "save_path = 'C:\\\\Users\\\\Mohammadreza\\\\Desktop\\\\My Class\\\\Proj-DC\\\\My Works\\\\My Papers\\\\intrusion\\\\data generator\\\\intrusion_detected'\n",
    "#plot_with_intrusions8(all_X_chunk_test, all_X_chunk_pred, all_intrusion_flags, sequence_length, save_path)\n",
    "\n",
    "jamming_detected = reconstruction_error > threshold1\n",
    "train_gen_instance.close()\n",
    "combined_gen_instance.close()\n",
    "#Table to get insight\n",
    "flattened_jamming_detected = jamming_detected.flatten()\n",
    "real_part_detected = jamming_detected[:, 0]\n",
    "imag_part_detected = jamming_detected[:, 1]\n",
    "\n",
    "real_true_count = np.sum(real_part_detected)\n",
    "real_false_count = len(real_part_detected) - real_true_count\n",
    "\n",
    "imag_true_count = np.sum(imag_part_detected)\n",
    "imag_false_count = len(imag_part_detected) - imag_true_count\n",
    "# Overall\n",
    "overall_true_count = np.sum(flattened_jamming_detected)\n",
    "overall_false_count = len(flattened_jamming_detected) - overall_true_count\n",
    "# Table-DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Part': ['Real', 'Imaginary', 'Overall'],\n",
    "    'True Count': [real_true_count, imag_true_count, overall_true_count],\n",
    "    'False Count': [real_false_count, imag_false_count, overall_false_count]\n",
    "})\n",
    "print(df)\n",
    "num_jamming_detected = np.sum(jamming_detected)\n",
    "print(f\"Number of jamming sequences detected: {num_jamming_detected} out of {len(flattened_jamming_detected)} sequences\")\n",
    "\n",
    "\n",
    "true_labels = np.array(true_labels).flatten()\n",
    "predicted_labels = np.array(predicted_labels, dtype=int).flatten()\n",
    "#predicted_labels = np.array(reconstruction_errors > threshold2, dtype=int).flatten()\n",
    "\n",
    "# Verify lengths and shapes\n",
    "print(\"Length of true labels:\", len(true_labels))\n",
    "print(\"Length of predicted labels:\", len(predicted_labels))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "try:\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    roc_auc = roc_auc_score(true_labels, predicted_labels)\n",
    "    fpr, tpr, _ = roc_curve(true_labels, predicted_labels)\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(f\"ROC AUC: {roc_auc}\")\n",
    "    print(report)\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Error in calculating metrics:\", e)\n",
    "\n",
    "\n",
    "# reconstruction error\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(reconstruction_error, label='Reconstruction Error')\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error with Threshold')\n",
    "plt.xlabel('Sequence Number')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('1-Reconstruction Error with Threshold.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "# reconstruction error\n",
    "reconstruction_error_real = reconstruction_error[:, 0]\n",
    "reconstruction_error_imag = reconstruction_error[:, 1]\n",
    "\n",
    "# Plot for Real Part\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(reconstruction_error_real, label='Reconstruction Error - Real Part', color='blue')\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error for Real Part with Threshold')\n",
    "plt.xlabel('Sequence Number')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('2-Reconstruction Error for Real Part with Threshold.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "# Plot for Imaginary Part\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(reconstruction_error_imag, label='Reconstruction Error - Imaginary Part', color='orange')\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error for Imaginary Part with Threshold')\n",
    "plt.xlabel('Sequence Number')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('3-Reconstruction Error for Imaginary Part with Threshold.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Histogram of Reconstruction Errors:\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.hist(reconstruction_error, bins=50, alpha=0.75)\n",
    "plt.axvline(x=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Histogram of Reconstruction Errors')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "# plt.savefig('4-Histogram of Reconstruction Errors.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Time Series Plot of IQ Samples:\n",
    "sample_index = np.random.choice(len(X_chunk_test))\n",
    "original_sample = X_chunk_test[sample_index]\n",
    "reconstructed_sample = X_chunk_pred[sample_index]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title('Original vs Reconstructed IQ Data')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('5-Original vs Reconstructed IQ Data.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "#Scatter Plot of Reconstruction Errors vs. Real and Imaginary Parts:\n",
    "avg_real = np.mean(X_chunk_test, axis=1)[:, 0]\n",
    "avg_imag = np.mean(X_chunk_test, axis=1)[:, 1]\n",
    "\n",
    "last_errors = np.mean(reconstruction_errors[-len(X_chunk_test):], axis=1)\n",
    "\n",
    "print(\"Shape of avg_real:\", avg_real.shape)\n",
    "print(\"Shape of avg_imag:\", avg_imag.shape)\n",
    "print(\"Shape of last_errors:\", len(last_errors))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(avg_real, last_errors, label='Real Part', alpha=0.5)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error vs. Average Real Part')\n",
    "plt.xlabel('Average Amplitude')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('6-Reconstruction Error vs. Average Real Part.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(avg_imag, last_errors, label='Imaginary Part', alpha=0.5)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error vs. Average Imaginary Part')\n",
    "plt.xlabel('Average Amplitude')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('7-Reconstruction Error vs. Average Imaginary Part.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "# # Define the number of sequences to plot together\n",
    "n = 5  # Change this to desired number of sequences\n",
    "sample_length = sequence_length * n\n",
    "\n",
    "# Select a random starting sequence for plotting\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "\n",
    "# Extract and concatenate the original and reconstructed samples\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "# Plot concatenated sequences\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('9-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "# Repeat for n = 9\n",
    "n = 9  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dcd153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for n = 9\n",
    "n = 9  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e566d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for n = 9\n",
    "n = 3  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc145640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction error\n",
    "reconstruction_error_real = reconstruction_error[:, 0]\n",
    "reconstruction_error_imag = reconstruction_error[:, 1]\n",
    "\n",
    "# Plot for Real Part\n",
    "plt.figure(figsize=(14, 6))\n",
    "mellow_green = '#89C997' \n",
    "plt.plot(reconstruction_error_real, label='Reconstruction Error', color=mellow_green)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Intrusion Detected by Reconstruction Error',fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Sequence Number (×10³)', fontsize=16, fontweight='bold')\n",
    "#plt.xlabel('Sequence Number(*1000)', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Reconstruction Error', fontsize=16, fontweight='bold')\n",
    "for label in (plt.gca().get_xticklabels() + plt.gca().get_yticklabels()):\n",
    "    label.set_fontsize(12)\n",
    "    label.set_fontweight('bold')\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f35dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695eecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbf99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
